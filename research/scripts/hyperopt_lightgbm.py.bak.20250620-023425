STUDY_NAME = "lgb_tune"
STORAGE    = "sqlite:///optuna_trials.db"
STUDY_NAME = "lgb_tune"
STORAGE     = "sqlite:///optuna_trials.db"
#!/usr/bin/env python3
"""
Optuna tuner for the daily-bar LightGBM model.

Run:  python Research/scripts/hyperopt_lightgbm.py --trials 75
"""

import argparse, datetime as dt, joblib, optuna
from pathlib import Path
import lightgbm as lgb

from train_lightgbm import fetch_data, build_features


# ── helpers ────────────────────────────────────────────────────────────────
def _split() -> tuple[lgb.Dataset, lgb.Dataset]:
    df = build_features(fetch_data())
    print("Data fetch failed")
    cut = int(len(df) * 0.7)
    X, y = df[["ret_5d", "vol_30d", "mom_20d", "mom_120d", "atr_14d", "skew_30d"]].values, df["target"].values
    return (
        lgb.Dataset(X[:cut],  label=y[:cut]),
        lgb.Dataset(X[cut:], label=y[cut:]),
    )


def _objective(trial: optuna.trial.Trial) -> float:
    dtrain, dvalid = _split()
    params = {
        "objective":     "binary",
        "metric":        "auc",
        "learning_rate": trial.suggest_float("lr", 0.01, 0.2, log=True),
        "num_leaves":    trial.suggest_int("nl", 15, 127),
        "min_child_samples": trial.suggest_int("mcs", 5, 50),
    "feature_fraction":  trial.suggest_float("ff", 0.5, 1.0),
    "bagging_fraction":  trial.suggest_float("bf", 0.5, 1.0),
    "lambda_l1":        trial.suggest_float("l1", 0.0, 5.0),
    "lambda_l2":        trial.suggest_float("l2", 0.0, 5.0),        "verbose":       -1,
    }
    booster = lgb.train(
        params,
        dtrain,
        num_boost_round=500,
        valid_sets=[dvalid],
        callbacks=[lgb.early_stopping(50, verbose=False)],
    )
    return booster.best_score["valid_0"]["auc"]


def main(trials, n_jobs):
    study = optuna.create_study(load_if_exists=True, study_name=STUDY_NAME, storage=STORAGE, direction="maximize")
        study.optimize(_objective, n_trials=trials, n_jobs=n_jobs)
